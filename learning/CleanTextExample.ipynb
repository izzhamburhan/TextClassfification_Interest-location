{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Izzham\n",
      "[nltk_data]     Burhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Izzham\n",
      "[nltk_data]     Burhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect  # Import the language detection module\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"This is a sample text with punctuation, numbers, and stopwords like the and in it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sample text with punctuation, numbers, and stopwords like the and in it!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sample text with punctuation numbers and stopwords like the and in it'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'with',\n",
       " 'punctuation',\n",
       " 'numbers',\n",
       " 'and',\n",
       " 'stopwords',\n",
       " 'like',\n",
       " 'the',\n",
       " 'and',\n",
       " 'in',\n",
       " 'it']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'text', 'punctuation', 'numbers', 'stopwords', 'like']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text punctuation numbers stopwords like\n"
     ]
    }
   ],
   "source": [
    "# Printing cleaned text\n",
    "cleaned_text = ' '.join(filtered_tokens)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "df_edu = pd.read_csv('../data/state/Data-education-state.csv',encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Going to a technology conference in Kuala Lumpur</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watching a football match in Johor Bahru</td>\n",
       "      <td>Johor</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trying a famous local food in Penang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attending a music concert in Kuala Lumpur</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Following the latest political news in KL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5064</th>\n",
       "      <td>Who remembers the old school Nintendo Game Boy...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5065</th>\n",
       "      <td>Hey there, rockstar medics! ?? Just wanted to ...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>In primary school, that DAMN monkey from dasav...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>Year 7s were today introduced to what their P...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>my day job is school and my hobby when i get h...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5069 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      location  \\\n",
       "0      Going to a technology conference in Kuala Lumpur  Kuala Lumpur   \n",
       "1              Watching a football match in Johor Bahru         Johor   \n",
       "2                  Trying a famous local food in Penang           NaN   \n",
       "3             Attending a music concert in Kuala Lumpur  Kuala Lumpur   \n",
       "4             Following the latest political news in KL           NaN   \n",
       "...                                                 ...           ...   \n",
       "5064  Who remembers the old school Nintendo Game Boy...  Kuala Lumpur   \n",
       "5065  Hey there, rockstar medics! ?? Just wanted to ...  Kuala Lumpur   \n",
       "5066  In primary school, that DAMN monkey from dasav...  Kuala Lumpur   \n",
       "5067  Year 7s were today introduced to what their P...  Kuala Lumpur   \n",
       "5068  my day job is school and my hobby when i get h...  Kuala Lumpur   \n",
       "\n",
       "           interest  \n",
       "0        Technology  \n",
       "1            Sports  \n",
       "2              Food  \n",
       "3     Entertainment  \n",
       "4          Politics  \n",
       "...             ...  \n",
       "5064      Education  \n",
       "5065      Education  \n",
       "5066      Education  \n",
       "5067      Education  \n",
       "5068      Education  \n",
       "\n",
       "[5069 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kena Reject Oleh University Di Malaysia Namun Diterima Harvard Dan UCLA. Ini Kisahnyaâ\\x80¦ ð\\x9f\\x91©ð\\x9f\\x8f»â\\x80\\x8dð\\x9f\\x8e\\x93ð\\x9f\\x91©ð\\x9f\\x8f»â\\x80\\x8dð\\x9f\\x8e\\x93  Baca artikel penuh disini &gt &gt  https://t.co/7zsnEYnWXC https://t.co/5qtodZjzES'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df_edu['text'][755]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use langdetect to detect the language of the text\n",
    "try:\n",
    "    language = detect(text)\n",
    "except:\n",
    "    language = \"unknown\"  # Handle cases where language detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-english : Kena Reject Oleh University Di Malaysia Namun Diterima Harvard Dan UCLA. Ini Kisahnyaâ¦ ð©ð»âðð©ð»âð  Baca artikel penuh disini &gt &gt  https://t.co/7zsnEYnWXC https://t.co/5qtodZjzES\n"
     ]
    }
   ],
   "source": [
    "if language == \"en\" :\n",
    "    print('english : ' + text)\n",
    "else :\n",
    "    print('non-english : ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interest-location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28840"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  location    interest\n",
      "3      hightech exports country china 942b hong kong ...       NaN  Technology\n",
      "18     hightech exports country china 942b hong kong ...       NaN  Technology\n",
      "163    communication tech service exports service exp...       NaN  Technology\n",
      "166    communication tech service exports service exp...       NaN  Technology\n",
      "226    bernamadotcom drsheikhmuszaph support thos typ...       NaN  Technology\n",
      "...                                                  ...       ...         ...\n",
      "28509           im jogging track pustaka kuching sarawak   Sarawak      Sports\n",
      "28522          im stapok badminton court kuching sarawak   Sarawak      Sports\n",
      "28549  im sentosa sports centre badminton court kuchi...   Sarawak      Sports\n",
      "28568           im jogging track pustaka kuching sarawak   Sarawak      Sports\n",
      "28596                                          badminton  Selangor      Sports\n",
      "\n",
      "[1242 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame without duplicates:\n",
      "                                                    text    location  \\\n",
      "0      attempt hari tu act like smart deep tech entre...         NaN   \n",
      "1      mosti cradle ni clueless whats going malaysias...         NaN   \n",
      "2      hightech exports country china 942b hong kong ...         NaN   \n",
      "4      tbh malaysia many tech talents dont actually n...         NaN   \n",
      "5      make 180000 tech consultant london grew extrem...         NaN   \n",
      "...                                                  ...         ...   \n",
      "28835  smartwatch one better sport ea garmin amazfit ...  Terengganu   \n",
      "28836              abearfromsea heaven football maracana  Terengganu   \n",
      "28837  king pele former youth sport minister khairykj...  Terengganu   \n",
      "28838                                      love football  Terengganu   \n",
      "28839  kb football clinic although internet connectio...  Terengganu   \n",
      "\n",
      "         interest  \n",
      "0      Technology  \n",
      "1      Technology  \n",
      "2      Technology  \n",
      "4      Technology  \n",
      "5      Technology  \n",
      "...           ...  \n",
      "28835      Sports  \n",
      "28836      Sports  \n",
      "28837      Sports  \n",
      "28838      Sports  \n",
      "28839      Sports  \n",
      "\n",
      "[27598 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Print or display the DataFrame without duplicates\n",
    "print(\"DataFrame without duplicates:\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27598"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "df_no_duplicates.to_csv('../data/cleaned_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
